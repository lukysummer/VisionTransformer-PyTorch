{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViT_Fine_Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TTCZNc_Z45fq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŒ´ **Created by** : Lucrece (Jahyun) Shin\n",
        "## ðŸŒ´ **Latest Edit** : January 28, 2022\n",
        "## ðŸŒ´ **Associated Blog Post** : "
      ],
      "metadata": {
        "id": "sRsUqLKW3yEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "u-5zUfvO5D_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time, os, pickle\n",
        "from PIL import Image, ImageFile\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import OrderedDict, deque\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import cv2 \n",
        "from google.colab.patches import cv2_imshow\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "GaddTf240nb7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import your Google drive if necessary"
      ],
      "metadata": {
        "id": "ARZYYK7e5Imb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zctVVRu5Hb_",
        "outputId": "3c857299-d805-4172-af6c-667156cd017d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import your data folders from your Google Drive"
      ],
      "metadata": {
        "id": "KQ6EmIoy3-G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell once\n",
        "!cp 'gdrive/My Drive/web_3cls+NOcutter.zip' . #web_2cls+benign.zip' . #'gdrive/My Drive/web_2cls+bag_SPLIT.zip' .\n",
        "!unzip -qq web_3cls+NOcutter.zip\n",
        "!rm web_3cls+NOcutter.zip \n",
        "\n",
        "!cp gdrive/MyDrive/Xray_2classes_cropped.zip .\n",
        "!unzip -qq Xray_2classes_cropped.zip\n",
        "!rm Xray_2classes_cropped.zip\n",
        "\n",
        "# 3 classes : gun, knife, benign\n",
        "!cp 'gdrive/My Drive/Xray-3cls_small.zip' .  # Xray-3cls_small_kitchen.zip\n",
        "!unzip -qq Xray-3cls_small.zip\n",
        "!rm Xray-3cls_small.zip \n",
        "# remove Kitchen knife & Cutter knife images\n",
        "!rm Xray-3cls_small/knife/Kitchen*\n",
        "!rm Xray-3cls_small/knife/Cutter*"
      ],
      "metadata": {
        "id": "6gzkKDHGm3iO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Dataloaders"
      ],
      "metadata": {
        "id": "kAco0ZQU5RUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define transforms\n",
        "transform = {\n",
        "        'train' : transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                      transforms.RandomHorizontalFlip(), \n",
        "                                      transforms.RandomRotation(50),\n",
        "                                      transforms.ToTensor()]),\n",
        "\n",
        "        'valid' : transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor()])\n",
        "        }"
      ],
      "metadata": {
        "id": "gPcrV7pM0p8i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `root_dir` is a string that contains the name of your source domain data folder. It must contain 3 folders named 'train', 'valid', 'test', each containing folders named by class names.\n"
      ],
      "metadata": {
        "id": "jEHhSCT0Cew5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"web_3cls+NOcutter/\" \n",
        "train_data = datasets.ImageFolder(root_dir + 'train', transform = transform['train'])\n",
        "valid_data = datasets.ImageFolder(root_dir + 'valid', transform = transform['valid'])\n",
        "test_data = datasets.ImageFolder(root_dir + 'test', transform = transform['valid'])\n",
        "print(\"Class2idx: \", train_data.class_to_idx)\n",
        "print('Train images :', len(train_data))\n",
        "print('Valid images :', len(valid_data))\n",
        "print('Test images :', len(test_data))\n",
        "num_workers = 0\n",
        "batch_size = 20\n",
        "dataloaders = {}\n",
        "dataloaders['train'] = DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "dataloaders['valid'] = DataLoader(valid_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "dataloaders['test'] = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
      ],
      "metadata": {
        "id": "p_vvqzMx01go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Multi-label Dataloaders (optional)\n",
        "To use multi-label dataset as discussed in [my blog post](https://medium.com/mlearning-ai/ch-6-optimizing-data-for-flexible-and-robust-image-recognition-23f4dcce3af7#f4d3), you must organize your folders in the following way:\n",
        "\n",
        "* [class a]+[class b] for images that contain both class a and b\n",
        "* [class a] for image that contain only class a\n",
        "\n",
        "For example, I originlly had 3 classes (gun, knife, benign) and made the following folders:\n",
        "\n",
        "* gun\n",
        "* gun+benign\n",
        "* knife\n",
        "* knife+benign\n",
        "* gun+knife\n",
        "* benign\n",
        "\n",
        "Here I am only considering images that contain at most 2 classes. Please adjust code for a situation where you must consider image containing more than 2 classes."
      ],
      "metadata": {
        "id": "TTCZNc_Z45fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLabelWebDataset(Dataset):\n",
        "  def __init__(self, root_dir, classes, transform=None, soft_label_class_name=None, soft_label=0.5):\n",
        "    # soft_label_class_name : name of the class that is given a soft label < 1\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    self.classes = classes\n",
        "    self.class_to_idx = {c:i for i, c in enumerate(self.classes)}\n",
        "    self.soft_label_class_name = soft_label_class_name\n",
        "    self.soft_label = soft_label  \n",
        "    self.data = self.make_dataset()                                        \n",
        "                                                      \n",
        "  def __len__(self):\n",
        "    return len(self.make_dataset())\n",
        "\n",
        "  def make_dataset(self):\n",
        "    instances = []\n",
        "    for target_class in os.listdir(self.root_dir):      \n",
        "      target_dir = os.path.join(self.root_dir, target_class)\n",
        "      # split up the class names by \"+\" sign\n",
        "      class_names = target_class.split(\"+\") # list of length 1 or 2\n",
        "      if not os.path.isdir(target_dir):\n",
        "        continue\n",
        "      for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
        "        for fname in sorted(fnames):  # for each image\n",
        "          label = [0]*len(self.classes)\n",
        "          path = os.path.join(self.root_dir, target_class, fname)\n",
        "\n",
        "          if len(class_names)==1:  # images that contain only one class\n",
        "            single_cls = class_names[0]\n",
        "            if single_cls==self.soft_label_class_name:\n",
        "              label[self.class_to_idx[single_cls]] = self.soft_label\n",
        "            else:\n",
        "              label[self.class_to_idx[single_cls]] = 1.\n",
        "\n",
        "          elif len(class_names)==2:  # images that contain two classes\n",
        "            for cls in class_names:\n",
        "              if cls==self.soft_label_class_name:\n",
        "                label[self.class_to_idx[cls]] = self.soft_label      \n",
        "              else:\n",
        "                label[self.class_to_idx[cls]] = 1.\n",
        "          item = path, label\n",
        "          instances.append(item)\n",
        "    return instances\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    path, target = self.data[idx]\n",
        "    image = Image.open(path).convert('RGB') \n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image, torch.tensor(target)"
      ],
      "metadata": {
        "id": "031aWZyI41t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `src_root_dir` is a string that contains the name of your source domain data folder. It must contain 3 folders named 'train', 'valid', 'test', each containing folders named by class names.\n",
        "* `soft_label` is a weaker label less than 1. Usually a class is given a label of 1 for a one-hot-encoded label. If a less significant class (if any) is recieving a strong signal from the model (i.e. large recall), you can set a soft label of e.g. 0.5 instead of 1 for that class."
      ],
      "metadata": {
        "id": "E8CJZyfa6vo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_root_dir = \"web_3cls+NOcutter\"## insert your data folder here ##\n",
        "classes=['benign','gun','knife']\n",
        "soft_label_class_name=\"benign\"\n",
        "soft_label=0.5 # weaker label < 1 \n",
        "train_data_multi_label = MultiLabelWebDataset(src_root_dir + '/train', classes=classes, transform = transform_, soft_label_class_name=soft_label_class_name, soft_label=soft_label)\n",
        "valid_data_multi_label = MultiLabelWebDataset(src_root_dir + '/valid', classes=classes, transform = transform_, soft_label_class_name=soft_label_class_name, soft_label=soft_label)\n",
        "test_data_multi_label = MultiLabelWebDataset(src_root_dir + '/test', classes=classes, transform = transform_, soft_label_class_name=soft_label_class_name, soft_label=soft_label)\n",
        "print(\"Class2idx: \", train_data_multi_label.class_to_idx)\n",
        "num_workers = 0\n",
        "batch_size = 16\n",
        "dataloaders_multi_label = {}\n",
        "dataloaders_multi_label['train'] = DataLoader(train_data_multi_label, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "dataloaders_multi_label['valid'] = DataLoader(valid_data_multi_label, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "dataloaders_multi_label['test'] = DataLoader(test_data_multi_label, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "print('Train images :', len(train_data_multi_label), \", # of training batches:\", len(dataloaders_multi_label['train']))\n",
        "print('Valid images :', len(valid_data_multi_label), \", # of valid batches:\", len(dataloaders_multi_label['valid']))\n",
        "print('Test images :', len(test_data_multi_label), \", # of test batches:\", len(dataloaders_multi_label['test']))"
      ],
      "metadata": {
        "id": "7CP6UYQH6r_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define ViT model from downloadable pre-trained checkpoints"
      ],
      "metadata": {
        "id": "AkcNY0It1HL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_collections\n",
        "!git clone https://github.com/jeonsworld/ViT-pytorch.git\n",
        "!mv ViT-pytorch/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm5pgCUdAfQs",
        "outputId": "80198359-1682-49e6-e5ed-e2c02f057bf2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml_collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–                           | 10 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 20 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 30 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 40 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 51 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 61 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from ml_collections) (1.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml_collections) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ml_collections) (1.15.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml_collections) (0.5.5)\n",
            "Building wheels for collected packages: ml-collections\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94524 sha256=b420a6cc3bebfeffe1f619d525527312301318f4c2b435aff63f1677bf416c41\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/da/64/33c926a1b10ff19791081b705879561b715a8341a856a3bbd2\n",
            "Successfully built ml-collections\n",
            "Installing collected packages: ml-collections\n",
            "Successfully installed ml-collections-0.1.1\n",
            "Cloning into 'ViT-pytorch'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 170 (delta 32), reused 27 (delta 27), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (170/170), 21.31 MiB | 29.05 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "from models.modeling import VisionTransformer, CONFIGS\n",
        "\n",
        "os.makedirs(\"model_checkpoints\", exist_ok=True)\n",
        "if not os.path.isfile(\"model_checkpoints/ViT-B_16-224.npz\"):\n",
        "    urlretrieve(\"https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/ViT-B_16-224.npz\", \n",
        "                \"model_checkpoints/ViT-B_16-224.npz\")\n",
        "\n",
        "config = CONFIGS[\"ViT-B_16\"]\n",
        "model = VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=True) \n",
        "model.load_from(np.load(\"model_checkpoints/ViT-B_16-224.npz\"))"
      ],
      "metadata": {
        "id": "HNEl6ogx0baw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Encoder & Classifier"
      ],
      "metadata": {
        "id": "R_8kfqxrQtW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = nn.Sequential(*[model.transformer.embeddings, model.transformer.encoder])\n",
        "ViT_embed_dim = 768  # final embedding dimension for ViT-B\n",
        "n_classes = 3        # number of classes for my dataset\n",
        "classifier = nn.Linear(ViT_embed_dim, n_classes)"
      ],
      "metadata": {
        "id": "6LlOWsjM0Xlp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp gdrive/MyDrive/encoder_1.pt .\n",
        "!cp gdrive/MyDrive/classifier_1.pt .\n",
        "encoder.load_state_dict(torch.load(\"encoder_1.pt\"))\n",
        "classifier.load_state_dict(torch.load(\"classifier_1.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbpQYo7mtBIP",
        "outputId": "ea07b3c7-8fe0-4378-8a2f-d0e744436359"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define ViT Fine-tuning Function"
      ],
      "metadata": {
        "id": "soXqtOFJBpWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPlz2DfIwPpt"
      },
      "outputs": [],
      "source": [
        "def fine_tune_ViT(encoder,     # ViT encoder (pre-trained)\n",
        "                  classifier,  # single-layer fully-connected classifier\n",
        "                  dataloaders, # dict with 2 keys, \"train\" and \"valid\", containing train & valid dataloaders\n",
        "                  n_epochs,    # number of epochs to fine-tune\n",
        "                  lr,          # learning rate\n",
        "                  multi_label_data=False\n",
        "                  ):\n",
        "  \n",
        "  encoder.cuda()\n",
        "  classifier.cuda()\n",
        "\n",
        "  #  1. Define optimizers and loss function  \n",
        "  optimizer_encoder    = optim.Adam(encoder.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "  optimizer_classifier = optim.Adam(classifier.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "  ### Different loss criterion for multi-label and single-label data\n",
        "  if multi_label_data:\n",
        "    criterion = nn.BCEWithLogitsLoss() \n",
        "  else: # single-label data (standard)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "  train_losses, val_losses, val_accs = [], [], []\n",
        "  for e in range(n_epochs):\n",
        "    #  2. Train on training data  \n",
        "    encoder.train()\n",
        "    classifier.train()\n",
        "    train_loss = 0.\n",
        "    for batch_i, (img, tgt) in enumerate(dataloaders['train']):\n",
        "      img, tgt = img.cuda(), tgt.cuda()\n",
        "\n",
        "      ## ViT encoder has 2 outputs, final embedding vectors for all image tokens and a stack of attention weights. ##\n",
        "      ## Here we are not using attention weights during training/validation. ##\n",
        "      ## embeddings: [batch_size, n_tokens, embedding dim]  e.g.[16, 197, 768] ##\n",
        "      embeddings, att_weights = encoder(img) \n",
        "\n",
        "      ## Extract [CLS] token (at index 0) 's embeddings used for classification ##\n",
        "      embedding_cls_token = embeddings[:, 0, :] # [batch_size, embedding dim] \n",
        "\n",
        "      logits = classifier(embedding_cls_token) # [batch_size, n_classes] \n",
        "      \n",
        "      optimizer_encoder.zero_grad()\n",
        "      optimizer_classifier.zero_grad()\n",
        "      if multi_label_data:\n",
        "        loss = criterion(logits.type(torch.FloatTensor), tgt.type(torch.FloatTensor))\n",
        "      else:\n",
        "        loss = criterion(logits.squeeze(-1).cuda(), tgt)\n",
        "      loss.backward()\n",
        "      optimizer_encoder.step()\n",
        "      optimizer_classifier.step()\n",
        "      train_loss += loss.item()\n",
        "\n",
        "    #  3. Evaluate on valdiation data  \n",
        "    encoder.eval()\n",
        "    classifier.eval()\n",
        "    val_loss = 0.\n",
        "    for batch_i, (img, tgt) in enumerate(dataloaders['valid']):\n",
        "      img, tgt = img.cuda(), tgt.cuda()\n",
        "      with torch.no_grad():\n",
        "        embeddings, att_weights = encoder(img) # embeddings: [batch_size, n_tokens, embedding dim]\n",
        "        embedding_cls_token = embeddings[:, 0, :]  # [batch_size, embedding dim]\n",
        "        logits = classifier(embedding_cls_token)   # [batch_size, n_classes]\n",
        "        loss = criterion(logits.type(torch.FloatTensor), tgt.type(torch.FloatTensor))\n",
        "        val_loss += loss.item()\n",
        "    \n",
        "    #  4. Log results and save model checkpoints \n",
        "    print(\"Epoch: {}/{}   Val CE Loss: {:.5f}\".format(e+1, n_epochs, val_loss/len(dataloaders['valid'])))\n",
        "    torch.save(encoder.state_dict(), 'encoder_{}.pt'.format(e+1))\n",
        "    torch.save(classifier.state_dict(), 'classifier_{}.pt'.format(e+1))\n",
        "\n",
        "  return encoder, classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune ViT!"
      ],
      "metadata": {
        "id": "98tQetLRBsD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder, classifier = fine_tune_ViT(encoder, \n",
        "                                    classifier, \n",
        "                                    dataloaders_multi_label, \n",
        "                                    n_epochs=4, \n",
        "                                    lr=3e-6,\n",
        "                                    multi_label_data=True)"
      ],
      "metadata": {
        "id": "Aqu7SGh87gfV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}